{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code is used to load the preprocess data for model training\n",
    "import numpy as np\n",
    "\n",
    "train_label = np.load('data/train_label.npy')\n",
    "train_categorical = np.load('data/train_categorical.npy') \n",
    "train_sparse = np.load('data/train_sparse.npy') \n",
    "train_numerical = np.load('data/train_numerical.npy')\n",
    "\n",
    "test_label = np.load('data/test_label.npy')\n",
    "test_categorical = np.load('data/test_categorical.npy') \n",
    "test_sparse = np.load ('data/test_sparse.npy') \n",
    "test_numerical = np. load ('data/test_numerical.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code is used to set the random seed\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 179493511\n",
    "\n",
    "def set_seeds(seed=SEED) :\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def set_global_determinism(seed=SEED):\n",
    "    set_seeds (seed=seed)\n",
    "\n",
    "    os.environt['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1) \n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Call the above function with seed value\n",
    "set_global_determinism(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as пр\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_probability as tfp\n",
    "import tqdm\n",
    "from typing import Sequence\n",
    "\n",
    "# install and import ltv\n",
    "#!pip install -q git+https://github.com/google/lifetime_value \n",
    "import lifetime_value as ltv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions\n",
    "%config InlineBackend. figure_format='retina'\n",
    "sns.set_style( 'whitegrid' )\n",
    "pd.options.mode.chained_assignment = None # default= 'warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following two losses would be used in the model optimization stage\n",
    "\n",
    "def mse(labels: tf.Tensor, logits: tf. Tensor) -› tf. Tensor:\n",
    "    labels = labels[:, 0]\n",
    "\n",
    "    label01 = tf.cast(tf.convert_to_tensor(labels, dtype=tf.float32) > 0, tf.float32)\n",
    "\n",
    "    loss = tf.reduce_mean(label01 * tf.math.square(labels-logits[:,0]))*tf.reduce_sum(tf.ones_like(labels))/tf.reduce_sum(label01) \n",
    "    print(loss.shape)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def cross(labels: tf.Tensor, logits: tf.Tensor) -> tf.Tensor:\n",
    "\n",
    "    label01 = 1 - tf.cast(tf.convert_to_tensor(labels[:,0], dtype=tf.float32) > 0, tf.float32)\n",
    "\n",
    "    return tf.reduce_mean(label01 * tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))*tf.reduce_sum(tf.ones_like(labels[:,0]))/tf.reduce_sum(label01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following code is used to load the pre-processed data, and these data are used to construct the collaborative filtlering feature\n",
    "\n",
    "data_dir = '/home/ma-user/work/ltv_pytorch-master/raw_data/'\n",
    "app_id_remap_dict = np.load(\n",
    "    f'{data_dir}preprocess/app_id_remap_dict.npy', allow_pickle=True).item() # the app_id_remap_dict is used to count the number of all paid apps.\n",
    "download_app_id_remap_dict = np.load(\n",
    "    f'{data_dir}preprocess/download_app_id_remap_dict.npy', allow_pickle=True).item() # the download_app_id_remap_dict is used to count the number of all download apps\n",
    "\n",
    "n_apps = max(app_id_remap_dict.values()) + 1\n",
    "n_down_apps = max(download_app_id_remap_dict.values()) + 1\n",
    "\n",
    "train_user_historical_downloads = np.load(\n",
    "    f'{data_dir}preprocess/remap_train_download_app_ids.npy') \n",
    "\n",
    "train_app_id = np.load(f'{data_dir}preprocess/remap_train_app_ids.npy') \n",
    "\n",
    "train_user_historical_download_mask_1 = np.load(\n",
    "    f' {data_dir}train_1/user._historical_downloads_mask.npy') \n",
    "train_user_historical_download_mask_2 = np.load(\n",
    "    f'{data_dir}train_2/user_historical_downloads_mask.npy') \n",
    "train_user_historical_download_mask = np.concatenate(\n",
    "    [train_user_historical_download_mask_1, train_user_historical_download_mask_2]) \n",
    "\n",
    "test_user_historical_downloads = \\\n",
    "    np.load(f'{data_dir})preprocess/remap_test_download_app_ids.npy') \n",
    "test_user_historical_download_mask = \\\n",
    "    np.load(f'{data_dir})test/user_historical_downloads_mask.npy') \n",
    "test_full_app_id = np.load(f'{data_dir}preprocess/test_full_app_id.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras. layers import Reshape, Embedding, Dense, Lambda, Input, Add, concatenate, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow. keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import initializers, regularizers\n",
    "tf. keras. backend.clear_session()\n",
    "init = initializers.RandomUniform(minval=-0.01, maxval=0.01)\n",
    "\n",
    "# the following code is used to model the linear relationship between the input features \n",
    "\n",
    "def featuresLinear(offset_dims, field_dims, x, input_mask):\n",
    "    fc = tf.keras.layers.Embedding(field_dims, 1, embeddings_initializer=init)\n",
    "    bias = tf.Variable(initial_value=tf.zeros((1,1)), trainabLe=True)\n",
    "    offsets = np.array((0, *np.cumsum(offset_dims)), dtype=np.long)\n",
    "\n",
    "    x = tf.cast(x, 'int32') + tf.expand_dims(tf.cast(offsets, 'int32'), axis=0)\n",
    "\n",
    "    y = fc(x) * tf.reshape(input_mask, (-1, 11, 1)) + bias\n",
    "\n",
    "    # <tf.Tensor 'add_1:0' shape=(None, 11, 1) dtype=float32>\n",
    "    return Flatten()(y)\n",
    "\n",
    "def featuresEmbedding(offset_mask, field_dims, n_factors, x, input_mask):\n",
    "    offsets = np.array((0, *np.cumsum(offset_dims)), dtype=np.long)\n",
    "    embedding = tf.keras.layers.Embedding(field_dims, n_factors, embeddings_initializer=init) \n",
    "    x = tf cast(x, 'int32') + tf.expand_dims(tf.cast(offsets, 'int32'), axis=0) \n",
    "    y = embedding(x) * tf.reshape(input_mask, (-1, 11, 1))\n",
    "\n",
    "    return y\n",
    "\n",
    "def fm(x, reduce_sum=False):\n",
    "    square_of_sum = K.sum(x, axis=1) ** 2\n",
    "\n",
    "    sum_of_square = K.sum(x ** 2, axis=1)\n",
    "    ix = square_of_sum - sum_of_square \n",
    "    if reduce_sum:\n",
    "        ix = K.sum(ix, axis=1, keepdims=True)\n",
    "    return 0.5 * ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_factors = 16\n",
    "field_dims = n_down_apps + n_apps\n",
    "offset_dims = [0] * 9\n",
    "offset_dims.append(n_down_apps)\n",
    "\n",
    "numeric_input = tf.keras.layers.Input(shape=(7,))\n",
    "categorical_input = tf.keras.layers.Input(shape=(37,))\n",
    "sparse_input = tf.keras.layers.Input(shape=(37,))\n",
    "\n",
    "user_input = tf.keras.layers.Input(shape=(10,))\n",
    "user_input_mask = tf.keras.layers.Input(shape=(10,))\n",
    "game_input = tf.keras.layers.Input(shape=(1, ))\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(22200, 16, embeddings_initializer=init) \n",
    "embedding_outputs = tf.keras.layers.Flatten()(embedding_layer(categorical_input))\n",
    "\n",
    "game_input_mask = Lambda(Lambda X: K.ones_like(x))(game_input) \n",
    "input_mask = tf.keras.layers.concatenate([user_input_mask, game_input_mask])\n",
    "\n",
    "x = tf.keras.layers.concatenate([user_input, game_input])\n",
    "\n",
    "x1 = featuresLinear(offset_dims, field_dims, x, input_mask)\n",
    "x2 = fm(featuresEmbedding(offset_dims, field_dims, n_factors, x, input_mask))\n",
    "interaction_embedding = tf.keras.layers.concatenate([x1, x2]) # this interaction_embedding can be treated as the collaborative filtering feature\n",
    "\n",
    "num_hidden = tf.keras.layers.BatchNormalization (axis=-1)(tf.keras.layers.Dense(64, activation='relu', kernel_initializer=init, bias_initializer=init) (numeric_input))\n",
    "\n",
    "#import pdb; pdb.set_trace()\n",
    "\n",
    "deep_input = tf.keras.layers.concatenate([num_hidden, interaction_embedding, embedding_outputs, sparse_input])\n",
    "\n",
    "hidden_input = tf.keras.layers.BatchNormalization(axis=-1)(tf.keras.layers.Dense(256, activation='relu', kernel_initializer=init, bias_initializer=init)(deep_input))\n",
    "hidden_input = tf.keras.layers.BatchNormalization (axis=-1)(tf.keras.layers.Dense(128, activation='relu', kernel_initializer=init, bias_initializer=init)(hidden_input))\n",
    "\n",
    "out_1 = tf. keras. layers.Dense(3, kernel_initializer=init, bias_initializer=init)(hidden_input)\n",
    "out_2 = tf.keras. layers.Dense(1, kernel_initializer=init, bias_initializer=init, activation='relu')(hidden_input) \n",
    "out_3 = tf.keras.layers.Dense(20, kernel_initializer=init, bias_initializer=init)(hidden_input)\n",
    "\n",
    "final_out = tf.keras.layers.concatenate([out_1,out_2, out_3])\n",
    "\n",
    "model = tf. keras.Model(inputs=[numeric_input, categorical_input, sparse_input,user_input, user_input_mask, game_input], outputs=[final_out, out_2, out_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following loss function is inspired by the ZILN loss function: Wang et al., A Deep Probabilistic Model for Customer Lifetime Value Prediction. \n",
    "# it is very important in the spending money prediction task\n",
    "def zero_inflated_gamma_loss(labels: tf.Tensor,\n",
    "                                    logits: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Computes the zero inflated lognormal loss.\n",
    "\n",
    "    Usage with tf.keras API:\n",
    "\n",
    "    ```python\n",
    "    model = tf. keras.Model(inputs, outputs)\n",
    "    model.compile( 'sgd', loss=zero_inflated _ Lognormal)\n",
    "    ```\n",
    "\n",
    "    Arguments:\n",
    "        Labels: True targets, tensor of shape [batch_size, 1]. \n",
    "        logits: Logits of output layer, tensor of shape [batch_size, 3].\n",
    "\n",
    "    Returns:\n",
    "        Zero inflated lognormal loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = tf.convert_to_tensor(labels, dtype=tf.float32)\n",
    "    labels = tf.squeeze(labels, axis=-1)\n",
    "\n",
    "    label01 = tf.cast(tf.convert_to_tensor(labels, dtype=tf.float32) > 0, tf.float32)\n",
    "\n",
    "    positive = tf.cast(labels > 0, tf.float32)\n",
    "\n",
    "    logits = tf.convert_to_tensor(logits, dtypp=tf.float32)\n",
    "\n",
    "    positive_logits = logits[..., 0]\n",
    "\n",
    "    classification_loss = tf.keras.losses.binary_crossentropy(y_true=positive, y_pred=positive_logits, from_logits=True)\n",
    "\n",
    "    alpha = tf.math.maximum(tf.nn.softplus(logits[:, 1]), 1e-4)\n",
    "    beta = tf.math.maximum(tf.nn.softplus(logits[:, 2]), 1e-4)\n",
    "\n",
    "    safe_labels = positive * labels + (\n",
    "        1 - positive) * tf.keras.backend.ones_like(labels)\n",
    "    regression_loss = -tf.keras.backend.mean (\n",
    "        positive * tfd.Gamma(concentration=alpha, rate=beta).log_prob(safe_labels), axis=-1)\n",
    "\n",
    "    label_reg = tf.math.log(1+ tf.math.maximum(labels, 0.))/tf.math.log(10.)\n",
    "    contrast_01mask = tf.expand_dims(label01,axis=1)*tf.expand_dims(label01,axis=0)\n",
    "    contrast_01negmask = tf.expand_dims(1-label01,axis=1)*tf.expand_dims(1-label01, axis=0)\n",
    "    contrast_01crossmask = tf.expand_dims(label01,axis=1)*tf.expand_dims(1-label01, axis=0)#+tf.expand_dims(1-label01,axis=1)*tf.expand_dims(label01, axis=0)\n",
    "\n",
    "    pred_miner = tf.expand_dims(logits[:,0],axis=1)-tf.expand_dims(logits[:,0],axis=0)\n",
    "\n",
    "    cross_pred_miner = 2 * tf.reduce_mean(tf.reduce_mean(pred_miner * contrast_01crossmask, axis=-1),axis=-1, keepdims=True)*\\ (tf.cast(tf.reduce_sum(tf.ones_like(labels),keepdims=True), tf.float32)**2/(tf.cast(tf.reduce_sum(tf.ones_Like(labels), keepdims=True), tf.float32) **2-tf.reduce_sum(label01, keepdims=True)**2-tf.reduce_sum(1-label01, keepdims=True)**2))\n",
    "\n",
    "    pred_miner_norm = tf.expand_dims(tf.sigmoid(logits[:,0]),axis=1)-tf.expand_dims(tf.sigmoid(logits[:,0]),axis=0) \n",
    "    alpha = tf.math.maximum(tf.nn.softplus(logits[:, 1]), 1e-4)\n",
    "    beta = tf.math.maximum(tf.nn.softplus(logits[:, 2]), 1e-4)\n",
    "\n",
    "    predict_gamma = tf.math.log(1+(alpha/beta))/tf.math.log(10.)\n",
    "\n",
    "    rec_reg = tf.math.log(tf.reduce_sum(tf.nn.softmax(logits[:, 4:])*(tf.cast(tf.ones_like(logits[:, 4:]), tf.float32)* tf.cast(2**tf.range(20), \n",
    "     tf.float32)),axis=-1))/tf.math.log (10.)\n",
    "    predreg_miner3 = tf.expand_dims(logits[:,3],axis=1)-tf.expand_dims(logits[:,3],axis=0)\n",
    "    predreg_miner = tf.expand_dims(tf.math.log(1+alpha/beta)/tf.math.log(10.),axis=1)-tf.expand_dims(tf.math.log(1+alpha/beta)/tf.math.log(10.),axis=0) cross_predreg_miner = tf.reduce_mean(tf.reduce_mean(tf.math.maximum(-predreg_miner*pred_miner_norm,0.)+tf.math.maximum (-predreg_miner3*pred_miner_norm,0.),axis=-1),axis=-1,keepdims=True)\n",
    "\n",
    "    loss_reg_mse = cross_predreg_miner - tf.reduce_mean(tf.math.log(tf.math.sigmoid(cross_pred_miner)))#+ cross_predreg_miner\n",
    "\n",
    "    print (cross_pred_miner.shape)\n",
    "    return 2*classification loss + regression_loss + loss_reg_mse\n",
    "\n",
    "loss = zero_inflated_gamma_loss#ltv.zero_inflated_lognormal_loss\n",
    "\n",
    "model.compile(loss=[loss, mse, cross], optimizer=keras.optimizers.Adam(1r=2e-4),loss_weights=[1., 1., 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [tf.keras.callbacks.ModelCheckpoint('weights.fepoch:02d}.hdf5', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, log_loss, r2_score, mean_squared_error, accuracy_score\n",
    "from scipy.stats import pearson, spearmanr\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow. keras.utils import to_categorical\n",
    "all_pred = []\n",
    "\n",
    "for i in range(5):\n",
    "    pos_idx_list = np.where(cost_value_array>0)[0]\n",
    "    neg_idx_List = random.sample(list(range(train_label.shape[0])), 4*pos_idx_list.shape[0])\n",
    "    sample_idx_list = np.concatenate([pos_idx_list, neg_idx_List])\n",
    "\n",
    "    model. fit([norm_train_numerical, train_categorical, train_sparse, train_user_historical_downloads, train_user_historical_download_mask, train_app_id], [train_label, np.expand_dims(np.log10(1+np.maximum(train_label,0.)),axis=1), to_categorical(np.floor(np.log2(1+np.maximum (train_label,0.))),20)], batch_size=10000,\n",
    "        epochs=2,\n",
    "        verbose=2, shuffle=True)#,callbacks=callbacks)\n",
    "    \n",
    "    logits, pred1, pred2 = model. predict([norm_test_numerical, test_categorical, test_sparse], batch_size=10000)\n",
    "\n",
    "    pred0 = (tf.keras.backend.sigmoid(logits[:,0])*tf.keras.backend.softplus(logits[..., 1])/tf.keras.backend.softplus(logits[..., 2])). numpy()\n",
    "    all_pred.append([logits, predi, pred2])\n",
    "    prob = sigmoid(logits[:,:1])\n",
    "\n",
    "    alpha = 0.3\n",
    "    beta = 0.2\n",
    "    preds = beta * pred0 + alpha*prob[:,0]*(10**pred1[:,0]-1) + (1-alpha- beta)*prob[:, 0]*(np.dot(scipy.special.softmax(pred2, axis=-1), 2**np.arange(20))-1)\n",
    "\n",
    "    # 获取排序后的索引\n",
    "    sorted_indices = sorted(range(len(preds)), key=lambda k: preds[k]) # 输出排序后的数据及其在原始列表中的索引 \n",
    "    for idx, i in enumerate (sorted_indices):\n",
    "        #print（collected_predictions［i］，\"在原始列表中的位置：“，i）\n",
    "        new_i = int(idx * len(ranking_cost_value_list) / len(sorted_indices)) \n",
    "        converted_predictions[i] = ranking_cost_value_list[new_i]\n",
    "\n",
    "    x_origin_test_labels = []\n",
    "    x_converted_predictions = []\n",
    "    for idx, value in enumerate(origin_test_labels):\n",
    "        if value > 0:\n",
    "            x_origin_test_labels.append(value)\n",
    "            x_converted_predictions.append(converted_predictions[idx])\n",
    "\n",
    "    r2 = r2_score(x_origin_test_labels, x_converted_predictions) \n",
    "    print(f'r2_score: {\"%.4f\"%r2}')\n",
    "\n",
    "    label, preds = test_label, preds\n",
    "\n",
    "    rows = np.shape(label)[0]\n",
    "    score_r2 = r2_score(label, preds)\n",
    "\n",
    "    pred_norm1 = np.linalg.norm(x=preds, ord=2)\n",
    "    true_norm = np.linalg.norm(x=label, ord=2)\n",
    "    sim = np.dot(preds.reshape((1, rows)), label.reshape((rows, 1)))/pred_norm1/true_norm\n",
    "    sim = sim[0, 0]\n",
    "    not_zero_index = np.where(label != 0.)[0]\n",
    "    score_r22 = r2_score(label[not_zero_index], preds[not_zero_index])\n",
    "\n",
    "    regression_auc = roc_auo_score(np.array(label> 0.,dtype='int32'), preds)\n",
    "    score_pearsonr_not_zero = pearsonr(label[not_zero_index], preds[not_zero_index].flatten())\n",
    "    score_spearmanr_not_zero = spearmanr(label[not_zero_index], preds[not_zero_index].flatten())\n",
    "    rmse = np.sqrt(mean_squared_error(label[not_zero_index], preds[not_zero_index].flatten()))\n",
    "    mae = mean_absolute_error(label[not_zero_index], preds[not_zero_index].flatten())\n",
    "    score_pearsonr = pearsonr(label, preds.flatten())\n",
    "    score_spearmanr = spearmanr(label, preds.flatten())\n",
    "    rmse2 = np.sqrt(mean_squared_error(label, preds.flatten()))\n",
    "    mae2 = mean_absolute_error(label, preds.flatten() )\n",
    "\n",
    "    # we adopt different kinds of metrics to evaluate the effectiveness of the proposed model, the score_r2 is the most important one. The smaller score_r2 denotes a better model. \n",
    "    print(score_r2, score_r22, score_pearsonr_not_zero[0], score_spearman_not_zero[0], score_pearsonr[0], score_spearmanr[0], rmse2, mae2, rmse, mse, regression_auc)\n",
    "\n",
    "    # in ths following code, we test the perfomance of the model with different outputs. \n",
    "    for preds in [pred0, prob[:, 0]*(10**pred1[:,0]-1), prob[:, 0]*(np.dot(scipy.special.softmax(pred2, axis=-1), 2**np.arange(20))-1)]:\n",
    "        rows = np.shape(label)[0]\n",
    "        score_r2 = r2_score(label, preds)\n",
    "\n",
    "        pred_norm1 = np.linalg.norm(x=preds, ord=2)\n",
    "        true_norm = np.linalg.norm(x=label, ord=2)\n",
    "        sim = np.dot(preds.reshape((1, rows)), label.reshape((rows, 1)))/pred_norm1/true_norm\n",
    "        sim = sim[0, 0]\n",
    "        not_zero_index = np.where(label != 0.)[0]\n",
    "        score_r22 = r2_score(label[not_zero_index], preds[not_zero_index])\n",
    "\n",
    "        regression_auc = roc_auc_score(np.array(Label> 0.,dtype='int32'), preds)\n",
    "        score_pearsonr_not_zero = pearsonr(label([not_zeno_index], preds[not_zero_index].flatten())\n",
    "        score_spearmanr_not_zero = spearmanr(label[not_zero_index], preds[not_zero_index].flatten())\n",
    "        rmse = np.sqrt(mean_squared_error(label[not_zero_index], preds[not_zero_index]. flatten() ))\n",
    "        mae = mean_absolute_error(label[not_zero_index], preds[not_zero_index].flatten() )\n",
    "        score_pearsonr = pearsonr(label, preds.flatten())\n",
    "        score_spearmanr = spearmanr(label, preds. flatten())\n",
    "        rmse2 = np.sqrt(mean_squared_error(label, preds.flatten() ))\n",
    "        mae2 = mean_absolute_error(label, preds.flatten())\n",
    "\n",
    "        print(score_r2, score_r22, score_pearson_not_zero[0], score_spearmann_not_zero[0], score_pearson[0], score_spearmann[0], rmse2, mae2, rmse, mae, regression_auc)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
